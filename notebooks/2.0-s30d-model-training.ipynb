{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9d9d89-4af7-4dc8-b1f3-c77dddad0c36",
   "metadata": {},
   "source": [
    "# Model training and prediction - `s30d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08843e3-1ca8-42ec-a344-1c5f560441dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4918d091-6dc7-47e1-8419-8973ebbf246c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ludvigwarnberggerdin/projects/ttris/pemett'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d758ca-f517-4df4-8bc0-f1cbc91226ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6ed398-04d7-4bba-8693-e79e973e5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/processed/s30d/X_train.csv\", index_col = 0)\n",
    "y_train = pd.read_csv(\"./data/processed/s30d/y_train.csv\", index_col = 0).s30d\n",
    "X_test = pd.read_csv(\"./data/processed/s30d/X_test.csv\", index_col = 0)\n",
    "y_test = pd.read_csv(\"./data/processed/s30d/y_test.csv\", index_col = 0).s30d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb610948-9c80-4507-861c-feeb3318fbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    94.197074\n",
       "1.0     5.802926\n",
       "Name: s30d, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts() / len(y_train.index) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe14dc61-2fe7-4188-9b39-5c4abb2f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = [\"age\", \"hr\", \"sbp\", \"dbp\", \"spo2\", \"rr\", \"delay\"]\n",
    "cat_features = list(X_train.loc[:, ~X_train.columns.isin(cont_features)].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d62c2b-0058-4997-8211-3cb704f90912",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084141d7-22d5-435b-b284-8e8eb769ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17e6786-1b75-4f11-a8d8-05cbe6d3e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cont', continous_transformer, cont_features)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb528c2-95b8-4505-801c-9fddf7a94935",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train.loc[:, cont_features] = ss.fit_transform(X_train.loc[:, cont_features])\n",
    "X_test.loc[:, cont_features] = ss.fit_transform(X_test.loc[:, cont_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9aca14-cc6c-49eb-8d39-6604121d1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7a154a-48d9-4936-bf5a-b6695a639991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludvigwarnberggerdin/miniforge3/envs/pemett/lib/python3.10/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    categorical_feature = cat_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1c960e-bbf2-4279-853c-f3d4207a7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_train = clf.predict_proba(X = X_train)\n",
    "y_pred_prob_test = clf.predict_proba(X = X_test)\n",
    "y_pred_train = clf.predict(X = X_train)\n",
    "y_pred_test = clf.predict(X = X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee16dc-8458-444d-b014-bc6852a5dfcd",
   "metadata": {},
   "source": [
    "Report for continous scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c592bf4a-44bf-40c5-a850-662b79c29caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      5860\n",
      "         1.0       1.00      0.99      1.00       361\n",
      "\n",
      "    accuracy                           1.00      6221\n",
      "   macro avg       1.00      1.00      1.00      6221\n",
      "weighted avg       1.00      1.00      1.00      6221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = y_train, y_pred = y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740c9e4e-0d5f-416e-9fb9-0a947c589c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98      1954\n",
      "         1.0       0.77      0.62      0.69       120\n",
      "\n",
      "    accuracy                           0.97      2074\n",
      "   macro avg       0.87      0.80      0.83      2074\n",
      "weighted avg       0.96      0.97      0.97      2074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = y_test, y_pred = y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b5800-760b-4d9b-8b96-c7ea65b7d47a",
   "metadata": {},
   "source": [
    "Gridsearch breaks for the continous score (to enable comparison with clinicians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f9e4e-ff18-4afd-bd68-28701e564246",
   "metadata": {},
   "source": [
    "## Run hyper parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e57514-909f-4bdb-83e4-6f83e4e7e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from src.models.train_model import generate_all_combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5af0873-c6fb-4b4d-9714-987c8bb04315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_inner_loop(base_clfs: list,\n",
    "                  inner_loop: Callable, \n",
    "                  X: pd.DataFrame, \n",
    "                  y: pd.Series,\n",
    "                  verbose: bool) -> np.ndarray:\n",
    "    \"\"\"Run inner loop of k-fold cross-validation.\n",
    "    \n",
    "    Uses sklearn's cross_val_predict.\n",
    "    \n",
    "    That is,\n",
    "    1. Fit classifier to the training folds.\n",
    "    2. Make prediction on the validation fold.\n",
    "    3. Use all folds as validation fold, one time each.\n",
    "\n",
    "    Args:\n",
    "      base_clfs: List of classifiers. E.g. [LGBMClassifier, LogisticRegression]\n",
    "      inner_loop: scikit-learn callable to split into folds\n",
    "      X: Features\n",
    "      y: Targets\n",
    "\n",
    "    Returns:\n",
    "      Each column represent predictions by each respective classifier\n",
    "    \"\"\"\n",
    "    predictions = np.zeros((len(X_train.index), ))\n",
    "    for clf in base_clfs:\n",
    "        if verbose: print(\"Running predictions for \" + str(clf))\n",
    "        preds = cross_val_predict(\n",
    "            estimator=clf,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            cv=inner_loop\n",
    "        )\n",
    "        if predictions.any():\n",
    "            predictions = np.hstack([predictions, preds[:, np.newaxis]])\n",
    "        else:\n",
    "            predictions = preds[:, np.newaxis]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b6e3a-596d-4f0f-9fcb-3a8db2cf35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingGeneralization():\n",
    "    \n",
    "\n",
    "    def __init__(self, base_clfs, meta_clf, verbose: bool = False):\n",
    "        self.base_clfs = base_clfs\n",
    "        self.meta_clf = meta_clf\n",
    "        \n",
    "        self.base_clfs__ = None\n",
    "       \n",
    "        inner_folds = 3\n",
    "        outer_folds = 2\n",
    "        self.inner_loop = StratifiedKFold(n_splits = inner_folds)\n",
    "        self.outer_loop = StratifiedKFold(n_splits = outer_folds)\n",
    "        \n",
    "      \n",
    "    def fit(self, hyper_parameters: list, \n",
    "            X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"Fits the classifiers and the meta classifier.\n",
    "    \n",
    "        Fits all the base classifiers, get predictions on all inner\n",
    "        loop validaion folds, and fit the meta classifiers to the predicted\n",
    "        probabilities.\n",
    "        \n",
    "        Args:\n",
    "            base_clfs: Base classifiers\n",
    "            meta_clf: Meta classifier\n",
    "            hyper_parameters: Hyper parameters for base classifiers and\n",
    "                breaks for binning continous predictions.\n",
    "            X_train: Training features.\n",
    "            y_train: Training targets.\n",
    "            verbose: If True, logging is printed in inner cross-validation.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of predicted proability of 1s and binned predictions.\n",
    "        \"\"\"\n",
    "        # Set the hyper hyper parameters of the base classifiers\n",
    "        for clfk in self.base_clfs.keys():\n",
    "            ks = [s for s in hyper_parameters.keys() if clfk in s]\n",
    "            clf_params = {k.split(\"__\")[1]: hyper_parameters.get(k) for k in ks}\n",
    "            clf = self.base_clfs[clfk]\n",
    "            clf.set_params(**clf_params)\n",
    "            self.base_clfs__.append(clf)\n",
    "        \n",
    "        # Get meta features of training set\n",
    "        meta_features_train = cv_inner_loop(\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "        )\n",
    "\n",
    "        # Fit meta classifier to meta features of train\n",
    "        meta_clf.fit(meta_features_train, y_train)\n",
    "    \n",
    "        return base_clfs_, meta_clf, meta_features_train\n",
    "    \n",
    "\n",
    "    def predict_meta_features(self, X):\n",
    "        per_model_preds = []\n",
    "        for clf in base_clfs__:\n",
    "            prediction = clf.predict_proba(X)[:, 1]\n",
    "            per_model_preds.append(prediction)\n",
    "        return np.hstack(per_model_preds)\n",
    "        \n",
    "\n",
    "    def predict(hyper_parameters: list,\n",
    "                X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                X_val: pd.DataFrame, verbose: bool = False) -> tuple:\n",
    "        \"\"\"Predicts using meta classifier.\n",
    "        \n",
    "        Use the fitted base classifiers to predict on the validation set,\n",
    "        fit the meta classifier to those predictions, and predict with \n",
    "        the meta classifier.\n",
    "        \n",
    "        Args:\n",
    "            base_clfs: Base classifiers.\n",
    "            meta_clf: Meta classifier.\n",
    "            hyper_parameters: Hyper parameters for base classifiers and\n",
    "                breaks for binning continous predictions.\n",
    "            inner_loop: Scikit-learn cross-validator. E.g. StratifiedKFold.\n",
    "            X_train: Training features,\n",
    "            y_train: Training targets.\n",
    "            X_val: Validation features.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of predicted proability of 1s and binned predictions.\n",
    "        \"\"\"\n",
    "        base_clfs_, meta_clf_, meta_features_train = self.fit(\n",
    "            hyper_parameters=hyper_parameters,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        meta_features_val = self.predict_meta_features(X)\n",
    "        # Fit meta classifier to meta features of train\n",
    "        meta_clf_.fit(meta_features_train, y_train)\n",
    "        # Predict using validation meta features\n",
    "        y_pred_con = meta_clf.predict_proba(meta_features_val)\n",
    "        # Calculate AUC of ROC for cut predictions\n",
    "        y_pred_cut = pd.cut(\n",
    "            x=y_pred_con[:, 1],\n",
    "            bins=hyper_parameters[\"breaks\"],\n",
    "            labels=[0, 1, 2, 3],\n",
    "            right=True,\n",
    "            include_lowest=False\n",
    "        )\n",
    "        return y_pred_con[:, 1], y_pred_cut\n",
    "\n",
    "    def cv_inner_loop(X_train: pd.DataFrame, y_train: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Run inner loop of k-fold cross-validation.\n",
    "    \n",
    "        Uses sklearn's cross_val_predict.\n",
    "        \n",
    "        That is,\n",
    "        1. Fit classifier to the training folds.\n",
    "        2. Make prediction on the validation fold.\n",
    "        3. Use all folds as validation fold, one time each.\n",
    "    \n",
    "        Returns:\n",
    "          Each column represent predictions by each respective classifier\n",
    "        \"\"\"\n",
    "        predictions = np.zeros((len(X_train.index), ))\n",
    "        for clf in self.base_clfs__:\n",
    "            if self.verbose: print(\"Running predictions for \" + str(clf))\n",
    "            preds = cross_val_predict(\n",
    "                estimator=clf,\n",
    "                X=X_train,\n",
    "                y=y_train,\n",
    "                cv=self.inner_loop\n",
    "            )\n",
    "            if predictions.any():\n",
    "                predictions = np.hstack([predictions, preds[:, np.newaxis]])\n",
    "            else:\n",
    "                predictions = preds[:, np.newaxis]\n",
    "    \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "402acec0-2fad-496e-afa8-a3b8057e529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(base_clfs: list, meta_clf: Callable,\n",
    "        inner_loop: Callable, hyper_parameters: list, \n",
    "        X_train: pd.DataFrame, y_train: pd.Series, \n",
    "        verbose: bool):\n",
    "    \"\"\"Fits the classifiers and the meta classifier.\n",
    "    \n",
    "    Fits all the base classifiers, get predictions on all inner\n",
    "    loop validaion folds, and fit the meta classifiers to the predicted\n",
    "    probabilities.\n",
    "    \n",
    "    Args:\n",
    "        base_clfs: Base classifiers\n",
    "        meta_clf: Meta classifier\n",
    "        hyper_parameters: Hyper parameters for base classifiers and\n",
    "            breaks for binning continous predictions.\n",
    "        X_train: Training features.\n",
    "        y_train: Training targets.\n",
    "        verbose: If True, logging is printed in inner cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of predicted proability of 1s and binned predictions.\n",
    "    \"\"\"\n",
    "    base_clfs_ = []\n",
    "    # Set the hyper hyper parameters of the base classifiers\n",
    "    for clfk in base_clfs.keys():\n",
    "        ks = [s for s in hyper_parameters.keys() if clfk in s]\n",
    "        clf_params = {k.split(\"__\")[1]: hyper_parameters.get(k) for k in ks}\n",
    "        clf = base_clfs[clfk]\n",
    "        clf.set_params(**clf_params)\n",
    "        base_clfs_.append(clf)\n",
    "    \n",
    "    # Get meta features of training set\n",
    "    meta_features_train = cv_inner_loop(\n",
    "        base_clfs=base_clfs_,\n",
    "        inner_loop=inner_loop,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Fit meta classifier to meta features of train\n",
    "    meta_clf.fit(meta_features_train, y_train)\n",
    "    \n",
    "    return base_clfs_, meta_clf, meta_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "28568e3c-98f4-416c-a9b1-e1046b991153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(base_clfs: list, meta_clf: Callable,\n",
    "            inner_loop: Callable, hyper_parameters: list,\n",
    "            X_train: pd.DataFrame, y_train: pd.Series,\n",
    "            X_val: pd.DataFrame, verbose: bool = False) -> tuple:\n",
    "    \"\"\"Predicts using meta classifier.\n",
    "    \n",
    "    Use the fitted base classifiers to predict on the validation set,\n",
    "    fit the meta classifier to those predictions, and predict with \n",
    "    the meta classifier.\n",
    "    \n",
    "    Args:\n",
    "        base_clfs: Base classifiers.\n",
    "        meta_clf: Meta classifier.\n",
    "        hyper_parameters: Hyper parameters for base classifiers and\n",
    "            breaks for binning continous predictions.\n",
    "        inner_loop: Scikit-learn cross-validator. E.g. StratifiedKFold.\n",
    "        X_train: Training features,\n",
    "        y_train: Training targets.\n",
    "        X_val: Validation features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of predicted proability of 1s and binned predictions.\n",
    "    \"\"\"\n",
    "    base_clfs_, meta_clf_, meta_features_train = fit(\n",
    "        base_clfs=base_clfs,\n",
    "        meta_clf=meta_clf,\n",
    "        inner_loop=inner_loop,\n",
    "        hyper_parameters=hyper_parameters,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get meta features of validation set\n",
    "    per_model_preds = []\n",
    "    for clf in base_clfs_:\n",
    "        clf.fit(X_train, y_train)\n",
    "        prediction = clf.predict_proba(X_val)[:, :-1]\n",
    "        per_model_preds.append(prediction)\n",
    "    meta_features_val = np.hstack(per_model_preds)\n",
    "    \n",
    "    # Fit meta classifier to meta features of train\n",
    "    meta_clf_.fit(meta_features_train, y_train)\n",
    "    # Predict using validation meta features\n",
    "    y_pred_con = meta_clf.predict_proba(meta_features_val)\n",
    "    # Calculate AUC of ROC for cut predictions\n",
    "    y_pred_cut = pd.cut(\n",
    "        x=y_pred_con[:, 1],\n",
    "        bins=hyper_parameters[\"breaks\"],\n",
    "        labels=[0, 1, 2, 3],\n",
    "        right=True,\n",
    "        include_lowest=False\n",
    "    )\n",
    "    return y_pred_con[:, 1], y_pred_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "104baafa-9465-4074-b79c-a6f2c44b8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_outer_loop(base_clfs: list, meta_clf: Callable,\n",
    "                  all_hyper_parameters: list,\n",
    "                  X: pd.DataFrame, y: pd.Series,\n",
    "                  use_meta_features: bool = False, \n",
    "                  verbose: bool = False):\n",
    "    \"\"\"Runs outer cross-validation.\n",
    "    \n",
    "    That is, find the best combination cut-points for the classifier.\n",
    "    \"Best\" is defined by the highest AUC of ROC.\n",
    "    \n",
    "    Inspired by:\n",
    "        https://github.com/rasbt/mlxtend/blob/master/mlxtend/classifier/stacking_cv_classification.py.\n",
    "        \n",
    "    Args:\n",
    "        base_clfs: Base classifiers.\n",
    "        meta_clf: Meta classifier.\n",
    "        all_hyper_parameters: Model hyper parameters and breaks for continous probabilities.\n",
    "        X: Features.\n",
    "        y: Targets.\n",
    "        use_meta_features: If True, the feature set for meta classifier is predicted probabilities\n",
    "            of positive labels from base classifiers + features used to train\n",
    "            base classifiers.\n",
    "        verbose: If True, logging is used in the inner cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        Three variables\n",
    "        - Refitted base classifiers,\n",
    "        - Refitted meta classifier, and \n",
    "        - The hyper parameters yielding the best results.\n",
    "    \"\"\"                \n",
    "    ## Setup splitting\n",
    "    inner_folds = 3\n",
    "    outer_folds = 2\n",
    "    inner_loop = StratifiedKFold(n_splits = inner_folds)\n",
    "    outer_loop = StratifiedKFold(n_splits = outer_folds)\n",
    "    \n",
    "    ## Setup for recording auc from each combination of hps\n",
    "    roc_aucs = pd.DataFrame(\n",
    "        data = np.zeros((len(all_hyper_parameters), outer_folds)),\n",
    "        columns = range(1, outer_folds + 1)\n",
    "    )\n",
    "    \n",
    "    for i, hyper_parameters in enumerate(all_hyper_parameters):\n",
    "\n",
    "        for j, (train_index, val_index) in enumerate(outer_loop.split(X, y)):\n",
    "        \n",
    "            X_train = X.iloc[train_index]\n",
    "            y_train = y.iloc[train_index]\n",
    "            X_val = X.iloc[val_index]\n",
    "            y_val = y.iloc[val_index]\n",
    "            \n",
    "            y_pred_con, y_pred_cut = predict(\n",
    "                base_clfs=base_clfs,\n",
    "                meta_clf=meta_clf,\n",
    "                hyper_parameters=hyper_parameters,\n",
    "                inner_loop=inner_loop,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_val=X_val,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            auc = roc_auc_score(\n",
    "                y_true=y_val,\n",
    "                y_score=y_pred_cut\n",
    "            )\n",
    "            roc_aucs.iloc[i, j] = 1 - auc\n",
    "\n",
    "    # Find the best performing settings for the models\n",
    "    max_row = roc_aucs.mean(axis=1).idxmax()\n",
    "    best_hyper_parameters = all_hyper_parameters[max_row]\n",
    "    \n",
    "    ## Refit to full training sample\n",
    "    # base_clfs_, meta_clf_ = base_clfs, meta_clf\n",
    "    # if refit: \n",
    "    #     base_clfs_, meta_clf_, _ = fit(\n",
    "    #         base_clfs=base_clfs,\n",
    "    #         meta_clf=meta_clf,\n",
    "    #         inner_loop=inner_loop,\n",
    "    #         hyper_parameters=hyper_parameters,\n",
    "    #         X_train=X_train,\n",
    "    #         y_train=y_train,\n",
    "    #         verbose=verbose\n",
    "    #     )\n",
    "    \n",
    "    return base_clfs_, meta_clf_, best_hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5ee3654e-7a80-4e7d-be40-7dfec004e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "all_breaks = [(0, ) + x + (np.inf,) for x in it.combinations(np.arange(0.01, 1, 0.01), r=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8bff7851-5b1f-4d4c-b729-272b7c698886",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    \"lgbm1__max_depth\": [100, 500],\n",
    "    \"lgbm1__num_leaves\": [200, 100],\n",
    "    \"lgbm2__max_depth\": [100, 200],\n",
    "    \"breaks\": all_breaks[:1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "27ae05c0-1ad4-4b86-8a15-b78d89c21982",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_clfs = {\n",
    "    \"lgbm1\": LGBMClassifier(),\n",
    "    \"lgbm2\": LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8e3662ba-f282-4633-af9a-9352ec551ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hyper_parameters = generate_all_combinations(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1d1a2ca9-96d1-4f79-b38c-e7448d06a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_clfs_, meta_clf_, best_hyper_parameters = cv_outer_loop(\n",
    "    base_clfs=base_clfs,\n",
    "    meta_clf=LogisticRegression(),\n",
    "    all_hyper_parameters=all_hyper_parameters,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb5220-b2db-4bec-8f9f-e3dad8166bf8",
   "metadata": {},
   "source": [
    "## Test the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b2b1042c-2d34-4851-ac55-130919db7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_con_holdout, y_pred_cut_holdout = predict(\n",
    "    base_clfs=base_clfs,\n",
    "    meta_clf=LogisticRegression(),\n",
    "    hyper_parameters=best_hyper_parameters,\n",
    "    inner_loop=StratifiedKFold(n_splits=3),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a3dcd300-91a0-4a71-83eb-41f77e69f2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hold-out continous model: 0.9497910269532583'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hold-out continous model: \" + str(1 - roc_auc_score(y_score = y_pred_con_holdout, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "77737ff6-8701-48af-a744-2b546ee87fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hold-out binned model: 0.678399010576595'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hold-out binned model: \" + str(1 - roc_auc_score(y_score = y_pred_cut_holdout, y_true = y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
